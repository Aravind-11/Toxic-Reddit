# -*- coding: utf-8 -*-
"""semi supervised learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t_BgJchTAJocx326YQ9j7IsfBxo4cJJD
"""

import pandas as pd
from google.colab import drive
drive.mount('/content/drive')
df=pd.read_csv(r"/content/drive/My Drive/physics/toxigen_data (1).csv")

from gensim.models.word2vec import Word2Vec
from gensim.models import KeyedVectors

#Embedding length based on selected model - we are using 50d here.
embedding_vector_length = 300

# Commented out IPython magic to ensure Python compatibility.

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from tqdm import tqdm
from sklearn.model_selection import train_test_split
import tensorflow as tf
from keras.models import Sequential
from keras.layers.recurrent import LSTM, GRU,SimpleRNN
from keras.layers.core import Dense, Activation, Dropout
from keras.layers.embeddings import Embedding
from tensorflow.keras.layers import BatchNormalization
from keras.utils import np_utils
from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline
from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D
from keras.preprocessing import sequence, text
from keras.callbacks import EarlyStopping

import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
from plotly import graph_objs as go
import plotly.express as ex
import plotly.figure_factory as ff

import re,string

def strip_links(text):
    link_regex    = re.compile('((https?):((//)|(\\\\))+([\w\d:#@%/;$()~_?\+-=\\\.&](#!)?)*)', re.DOTALL)
    links         = re.findall(link_regex, text)
    for link in links:
        text = text.replace(link[0], ', ')    
    return text

list1=list(df['class'].unique())
dict={}
for i in range(len(list1)):
  dict[list1[i]]=i

for i in df.index:
  df['class'][i]=dict[df['class'][i]]

from sklearn.model_selection import train_test_split

train, test = train_test_split(df, test_size=0.2)

max_features = 5000
maxlen = 500

train.text=train.text.astype('str')
test.text=test.text.astype('str')

train.text

token=tf.keras.preprocessing.text.Tokenizer(num_words=50000)
token.fit_on_texts(train.text)

tf.config.list_physical_devices ('GPU')

train_seq=token.texts_to_sequences(train.text)
test_seq=token.texts_to_sequences(test.text)

print(train_seq[0])#generating sequences

#zero pad the sequences
train_pad = sequence.pad_sequences(train_seq, maxlen=maxlen)
test_pad = sequence.pad_sequences(test_seq, maxlen=maxlen)

df2=pd.read_csv('check.csv')

df2['body']=df2['0']

# Define the function to remove the punctuation
def remove_punctuations(text):
    for punctuation in string.punctuation:
        text = text.replace(punctuation, '')
    return text
# Apply to the DF series
df2['body']= df2['body'].apply(remove_punctuations)

text1=token.texts_to_sequences(df2.body)
#test_seq=token.texts_to_sequences(test.text)

#zero pad the sequences
text1 = sequence.pad_sequences(text1, maxlen=500)
#test_pad = sequence.pad_sequences(test_seq, maxlen=maxlen)

train_onehot_encoded = []
for value in train['class']:
	letter = [0 for _ in range(27)]
	letter[value] = 1
	train_onehot_encoded.append(letter)
test_onehot_encoded = []
for value in test['class']:
	letter = [0 for _ in range(27)]
	letter[value] = 1
	test_onehot_encoded.append(letter)

len(train_pad)

train_onehot_encoded=np.array(train_onehot_encoded)
test_onehot_encoded=np.array(test_onehot_encoded)

n=0
index=[]
for i in range(len(test)):
  if max(test[i])>=0.80:
    index.append(i)
    n+=1
#print(n/len(test))

type(train_onehot_encoded)

from tensorflow import keras
model = keras.models.load_model('my model-2.h5')

b=list(train_pad)
type(b[0])

type(text1[i])

num_iters=10
eps=0.0
for _ in range(num_iters):
    train_pad=list(train_pad)
    train_onehot_encoded=list(train_onehot_encoded)
    test1=model.predict(text1)
    
    index=[]
    for i in range(len(test1)):
      if max(test1[i])>=(0.85+eps):
        index.append(i)
    for i in index:
      add=text1[i]

      #np.append(train_pad,text1[i])
      #print(type(train_pad))
      train_pad.append(text1[i])
      
      
      value=test1[i].argmax()
      letter = [0 for _ in range(27)]
      letter[value]=1
      
      train_onehot_encoded.append(letter)
      
      #np.append(train_onehot_encoded,letter)
    train_pad=np.array(train_pad)
    train_onehot_encoded=np.array(train_onehot_encoded)
    callbacks=[EarlyStopping(monitor='val_accuracy',patience=3)]
    history = model.fit(train_pad,
                        train_onehot_encoded,
                        epochs=20,
                        batch_size=32,          
                        validation_data=(test_pad, test_onehot_encoded),callbacks=callbacks)
    eps+=0.12

model.save(' my new model.h5')

from google.colab import files

files.download(' my new model.h5')

len(train_onehot_encoded)





from tensorflow import keras
model = keras.models.load_model('my model-2.h5')

callbacks=[EarlyStopping(monitor='val_accuracy',patience=3)]
history = model.fit(train_pad,
                    train_onehot_encoded,
                    epochs=25,
                    batch_size=32,          
                    validation_data=(test_pad, test_onehot_encoded),callbacks=callbacks)

test=model.predict(text1)

avg=0
for i in range(len(test)):
  avg+=max(test[i])
print(avg/len(test))

n=0
index=[]
for i in range(len(test)):
  if max(test[i])>=0.80:
    index.append(i)
    n+=1
print(n/len(test))

for i in index:
  print(df2['body'][i])
  break

inverse_dict={}
for i in dict:
  inverse_dict[dict[i]]=i
  #print(i,dict[i])
print(inverse_dict)

list1=[]
for i in index:
  list1.append([df2['body'][i],test[i].argmax()])

list1[0]

df.columns

for i in list1:
  df['text'].append(list1[i][0])
  df['class'].append(list1[i][1])

for i in range(len(list1)):
  df=df.append({'Unnamed: 0':0,'text':list1[i][0],'class':list1[i][1]},ignore_index=True)

df

